\documentclass{article}
\usepackage{acl2013}
\usepackage[utf8]{inputenc}
%\usepackage{natbib}
\usepackage{times}
%\usepackage{url}
\usepackage{color}
\usepackage{multirow}
%\usepackage{latexsym}
\usepackage{booktabs,amsmath,multicol}
\usepackage[all]{xy}
\usepackage{dblfloatfix}
\usepackage{tikz}
%\setlength\titlebox{6.5cm}    % You can expand the title box if you
% really have to
% \textheight 685pt 
% \hyphenation{Buda-pest}
% \addtocounter{section}{-1}

\title{Finite-state modeling of general vocabulary}

% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}

\author{D\'avid Nemeskey\\
 HAS Computer and Automation Research Institute, \\
   Kende u 13-17, \\
   H-1111 Budapest\\
 {\tt nemeskey.david@sztaki.mta.hu}
\And
Francis M. Tyers\\
Dept. Lleng. i Sist. Inform.,\\
Universitat d'Alacant,\\
E-03080 Alacant\\
{\tt ftyers@dlsi.ua.es}}

\date{}

\begin{document}
\maketitle\vspace*{-10mm}

\begin{abstract}
  % TODO Add Yli-Jyra:2011
  In recent years, the problem of finite state Constraint Grammar~(CG) parsing has
  received renewed attention. Both \shortcite{Hulden:2011} and
  \shortcite{Peltonen:2011} presented compilers that convert CG rules to finite
  state transducers. While these formalisms serve their purpose as proofs of the
  concept, their performance lags behind other implementations.  %TODO implementionS?
  % TODO I am not talking about compilation, but application
  
  In this paper, we argue that fault lies with using generic finite state
  libraries, and not with the formalisms themselves. We present an open-source
  implementation that capitalizes on the characteristics of CG rule
  application to improve execution time. The implementation achieves performance
  comparable to the current open-source state of the art.
  % TODO OMG this is horrible
\end{abstract}

\section{Introduction}
Outline:
- CG
- FST implementations
- slow or no performance measures
- generic library, implementation can matter
- why we chose Hulden and not Peltonen (application + composition vs intersection)
- 
% TODO: related work: Mans (not really "related"), Peltonen, Yli-Jyra
In the rest of the paper, we present our optimizations in a way that mirrors the
actual development process. We start out with a simple rule engine based on
foma, and improve it step-by-step, benchmarking its performance after each
modification, instead of a single evaluation chapter. We start in
section~\ref{sec:methodology} by describing our evaluation methodology.
Section~\ref{sec:speed} follows the evolution of the rule engine, as it improves
in terms of speed. Section~\ref{sec:idea} introduces an idea that theoretically
allows us to improve the $\mathcal{O}(Gn^2k^2)$ worst-case asymptotic bound.
Section~\ref{sec:memory} demonstrates how memory savings can be saved from the
steps taken in section~\ref{sec:speed}. Finally, section \ref{sec:conclusion}
contains our conclusions and lists the problems that remain for future work.

%\section{Bridging the Gaps}
%\label{sec:bridging}
%% TODO: what Mans did not cover in his paper (IFF, delimiters)?

\section{Evaluation Methodology}
\label{sec:methodology}
% TODO mention the Hungarian grammar before this paragraph!
The performance of \texttt{fomacg-proc} has been measured against that of
\texttt{cg-proc}, VISL-GC's rule applier. The programs were benchmarked with
three Apertium CG grammars: the toy Hungarian grammar mentioned earlier, the
Breton grammar from the \texttt{br-fr} language pair and the version of the
Finnish grammar due to Karlsson in the \texttt{sme-fin} pair. % TODO: what is sme? Citations!

Each grammar was run on a test corpus. For Breton, we used the corpus
in the \texttt{br-fr} language pair, which consists of 1,161 sentences. There are
no Finnish and Hungarian corpora in Apertium; for the former, we used a
1,620-sentence excerpt from the 2013-Nov-14 snapshot of the Finnish Wikipedia,
while for the latter, the short test corpus used for grammar development. Since
the latter contains a mere 11 sentences, it was repeated 1024 times to produce
a corpus similar in size to the other two.

CG parsers expect morphologically analyzed input. The Breton and Finnish corpora
were tagged by Apertium's morphological analyzer tool. As Hungarian is not
properly supported by Apertium yet, we used Hunmorph\cite{Tron:2005}, and
translated the tags to the Apertium tagset with a transducer in \texttt{foma}.

Since VISL-GC implements CG-3, and fomacg only supports CG-2, a one-to-one
comparison with the grammars above was infeasible. Therefore, we extracted the
subset of rules from each that compiled under fomacg, and carried out the tests
on these subsets. Table~\ref{tab:grammar_size} shows the number of rules in the
original and the CG-2 grammars.

We recorded both initialization and rule application time for the two programs,
via instrumentation in case of \texttt{fomacg-proc} and by running the grammar
first on an empty file and then on the test corpus in case of \texttt{cg-proc}.
However, as initialization is one-time cost, in the following we are mainly
concerned with the time required for applying rules. The tests were conducted on
a consumer-grade laptop with a 2.2GHz Core2Duo CPU and 4GB RAM, running Linux.
% TODO rewrite this a bit

\section{Performance Optimizations}
\label{sec:speed}

\section{Beyond the $\mathcal{O}(Gn^2k^2)$ Bound}
\label{sec:idea}

\section{Memory Savings}
\label{sec:memory}

\subsection{Token Matching}
\label{sec:sub_token}

The transition from text to integer representation of the $\Sigma$ alphabet
has also opened a way to decrease the storage space requirements of the grammar.
Once again, the improvement stems not from theoretical, but practical
considerations, and it concerns the conversion of text into tokens in $\Sigma$.
Usually this step is taken for granted in the literature, but implementations
must allocate resources to tackle this task.  % in their own way.

% TODO describe the trie (e.g. byte-based, 256-way, etc.)
In \texttt{foma}, token matching is performed by a trie built from the symbols
in the automaton's alphabet. Depending on the number and length of the symbols
in bytes, this trie may be responsible for a considerable portion of the memory
footprint of an automaton. Given the number of rules in an average CG grammar,
it is easy to see how this trivial sub-task may affect the memory consumption of
the application, as well as the size of the grammar binary.

% TODO What we could do

% TODO delete conditions on the right branches in condition trees

\begin{table*}[h]
  \centering
  \caption{Grammar sizes with the running time and binary size of the respective
           VISL-GC grammars}
  \label{tab:grammar_size}
  \begin{tabular}{ | l | r | r | r | r | }
  \hline
  \textbf{Language} & \textbf{Rules} & \textbf{CG-2 rules} &
  \textbf{Binary} & \textbf{Time} \\
  \hline
  Hungarian & &   35 &   8kB & 0.3s \\
  Breton    & &  226 &  36kB & 0.8s \\
  Finnish   & & 1172 & 184kB & - \\    % TODO: test sme-fin for running time!!!
  \hline
  \end{tabular}
\end{table*}

\begin{table}[h]
  \centering
  \caption{Improvements in memory usage due to removing the sigma trie. Memory
           consumption is measured as a percentage of the 4GB system memory}
           % TODO: explain what Lx means HERE?
  \label{tab:sigma_memory}
  \begin{tabular}{ | l | r | r | r | }
  \hline
  \textbf{Language} & \textbf{Before} & \textbf{After} & \textbf{Reduction} \\
  \hline
  hun L1 & 0.5\% & 0.1\% & 80\% \\
  hun L3 & 2.1\% & 1.5\% & 28.57\% \\
  br L1 & 5.1\% & 1.3\% & 74.5\% \\
  br L2 & 9.6\% & 4.4\% & 54.16\% \\
  fin L1 & 21\% & 4.1\% & 80.47\% \\
  fin L2 & 32.3\% & 8.9\% & 72.44\% \\
  \hline
  \end{tabular}
\end{table}

\section{Conclusions}
\label{sec:conclusion}
Outline:
\begin{itemize}
\item Implementation matters.
\item We've made some real advances on the state-of-the-art free/open-source
  FST implementations of CG. 
\item Here we need to describe how we are going to reduce the size problem.
\end{itemize}
% TODO: Read and add Yli-Jyra:2011 -- a different method (I also was thinking
%       about a two-step method, where the first level converts cohorts to 
%       some representation (the sets it contains, etc.), and the second level
%       does something else).

\section*{Acknowledgements}
This research was conducted under the Google Summer of Code 2013 project
Rule-based finite-state disambiguation\footnote{https://google-melange.appspot.com/gsoc/project/google
/gsoc2013/davidnemeskey/14002}.

% TODO: citations:
\cite{Karlsson:1990} % Karlsson:1990: the Finnish grammar
\cite{Hulden:2011} % Hulden:2011: Mans's paper
\cite{Peltonen:2011} % Peltonen:2011: the other try... should we include it at all?
\cite{Tapanainen:1996} % Tapanainen:1996: CG-2

\bibliographystyle{acl}
\bibliography{apertium_cg}
\end{document}
